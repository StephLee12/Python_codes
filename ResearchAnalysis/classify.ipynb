{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('usualEnv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ffce47e96513169ae8a63192f15a1bcf444fe27eb030511ca4e393e94d40ac6b"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df = df[['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500','IfShoeLover']]\n",
    "for col_name in ['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500','IfShoeLover']:\n",
    "    if col_name == 'IfShoeLover':\n",
    "        for row_index in range(df.shape[0]):\n",
    "            if df.loc[row_index,col_name] == 2:\n",
    "                df.loc[row_index,col_name] = 0\n",
    "        continue\n",
    "    if col_name == 'IfAcceptService':\n",
    "        for row_index in range(df.shape[0]):\n",
    "            if df.loc[row_index,col_name] == 2:\n",
    "                df.loc[row_index,col_name] = 0\n",
    "        continue\n",
    "\n",
    "    for row_index in range(df.shape[0]):\n",
    "        if df.loc[row_index,col_name] == 1:\n",
    "            df.loc[row_index,col_name] = 0\n",
    "        else:\n",
    "            df.loc[row_index,col_name] = 1\n",
    "\n",
    "data = df[['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500']].values\n",
    "target = df['IfShoeLover'].values\n",
    "\n",
    "train_set,test_set = df[0:150],df[150:].reset_index()[['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500','IfShoeLover']]\n",
    "\n",
    "train_set_data = df.loc[0:149,['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500']]\n",
    "train_set_target = df.loc[0:149,['IfShoeLover']]\n",
    "\n",
    "test_set_data = df.loc[150:,['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500']].reset_index()[['IfAcceptService','LessThan500','500To1000','1000To1500','NoLessThan1500']]\n",
    "test_set_target = df.loc[150:,['IfShoeLover']].reset_index()['IfShoeLover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6530612244897959"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "# KNN直接调包\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=2)\n",
    "classifier.fit(train_set_data,train_set_target)\n",
    "classifier.score(test_set_data,test_set_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.673469387755102\n[[ 4. 12.]\n [ 4. 29.]]\n"
     ]
    }
   ],
   "source": [
    "# 手写KNN\n",
    "# vote\n",
    "def knn_vote(best_k,train_df,columns):\n",
    "      \n",
    "    vote_list = train_df.loc[best_k,columns[-2]]\n",
    "    mode = vote_list.mode().get(0)\n",
    "    return mode\n",
    "\n",
    "# calculate distance\n",
    "def cal_dis(vec_train,vec_test):\n",
    "    \n",
    "    vec_train,vec_test = np.array(vec_train), np.array(vec_test)\n",
    "    dis = np.sqrt(np.sum(np.square(vec_train - vec_test)))\n",
    "    return dis\n",
    "\n",
    "def cal_correct_rate(test_df,columns):\n",
    "    \n",
    "    # 构建混淆矩阵\n",
    "    labels = list(test_df[columns[-2]].unique())\n",
    "    mat_shape = len(labels)\n",
    "    conf_mat = np.zeros([mat_shape,mat_shape])\n",
    "    \n",
    "    correct_sum = 0\n",
    "    for i in range(test_df.shape[0]):\n",
    "        true_label_idx = labels.index(test_df.at[i,columns[-2]])\n",
    "        pred_label_idx = labels.index(test_df.at[i,columns[-1]])\n",
    "        conf_mat[true_label_idx][pred_label_idx] += 1\n",
    "        if test_df.at[i,columns[-2]] == test_df.at[i,columns[-1]]:\n",
    "            correct_sum += 1\n",
    "    return conf_mat,correct_sum / test_df.shape[0]\n",
    "\n",
    "# knn main function\n",
    "def knn_classify_main(train_df,test_df,columns,k=2):\n",
    "    \n",
    "    # add a new column to record classification result\n",
    "    columns.append('class_classify')\n",
    "    test_df.insert(len(columns)-1,'class_classify','')\n",
    "    \n",
    "    for i in range(test_df.shape[0]):\n",
    "        # get test_data vector\n",
    "        vec_test = []\n",
    "        for ii in range(len(columns) -2):\n",
    "            vec_test.append(test_df.iat[i,ii])\n",
    "        \n",
    "        best_k , best_k_dis = [], []\n",
    "        for j in range(train_df.shape[0]):\n",
    "            # get train_data vector\n",
    "            vec_train = []\n",
    "            for jj in range(len(columns) -2):\n",
    "                vec_train.append(train_df.iat[j,jj])\n",
    "            # calculate distance\n",
    "            dis = cal_dis(vec_train,vec_test)\n",
    "            if j < k: # add k initial elems\n",
    "                best_k.append(j)\n",
    "                best_k_dis.append(dis)\n",
    "            else: # substitute elem\n",
    "                for kk in range(len(best_k)):\n",
    "                    if dis < best_k_dis[kk]:\n",
    "                        best_k[kk] = j\n",
    "                        best_k_dis[kk] = dis\n",
    "                        break   \n",
    "        \n",
    "        vote_res = knn_vote(best_k,train_df,columns)\n",
    "        test_df.at[i,columns[-1]] = vote_res\n",
    "       \n",
    "    conf_mat,correct_rate = cal_correct_rate(test_df,columns)\n",
    "    return test_df,conf_mat,correct_rate\n",
    "columns = ['LessThan500','500To1000','1000To1500','NoLessThan1500','IfShoeLover']\n",
    "train_data,test_data = train_set,test_set\n",
    "test_df,conf_mat,correct_rate = knn_classify_main(train_data,test_data,columns,k=2)\n",
    "print(correct_rate)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45228385925\n",
      "Loss is 0.2131698578596115\n",
      "Loss is 0.21316847205162048\n",
      "Loss is 0.213166743516922\n",
      "Loss is 0.213165283203125\n",
      "Loss is 0.21316386759281158\n",
      "Loss is 0.21316233277320862\n",
      "Loss is 0.21316099166870117\n",
      "Loss is 0.21315963566303253\n",
      "Loss is 0.21315820515155792\n",
      "Loss is 0.21315675973892212\n",
      "Loss is 0.2131555676460266\n",
      "Loss is 0.21315419673919678\n",
      "Loss is 0.21315275132656097\n",
      "Loss is 0.21315127611160278\n",
      "Loss is 0.21315020322799683\n",
      "Loss is 0.2131485491991043\n",
      "Loss is 0.21314747631549835\n",
      "Loss is 0.2131463587284088\n",
      "Loss is 0.2131449431180954\n",
      "Loss is 0.2131435126066208\n",
      "Loss is 0.213142529129982\n",
      "Loss is 0.2131413221359253\n",
      "Loss is 0.21314001083374023\n",
      "Loss is 0.21313871443271637\n",
      "Loss is 0.21313773095607758\n",
      "Loss is 0.2131366729736328\n",
      "Loss is 0.21313533186912537\n",
      "Loss is 0.21313416957855225\n",
      "Loss is 0.21313299238681793\n",
      "Loss is 0.21313200891017914\n",
      "Loss is 0.21313075721263885\n",
      "Loss is 0.21312952041625977\n",
      "Loss is 0.21312858164310455\n",
      "Loss is 0.21312746405601501\n",
      "Loss is 0.21312640607357025\n",
      "Loss is 0.21312536299228668\n",
      "Loss is 0.21312451362609863\n",
      "Loss is 0.21312347054481506\n",
      "Loss is 0.21312229335308075\n",
      "Loss is 0.2131211906671524\n",
      "Loss is 0.21312031149864197\n",
      "Loss is 0.2131192833185196\n",
      "Loss is 0.21311841905117035\n",
      "Loss is 0.2131175696849823\n",
      "Loss is 0.2131163626909256\n",
      "Loss is 0.213115394115448\n",
      "Loss is 0.21311461925506592\n",
      "Loss is 0.2131136953830719\n",
      "Loss is 0.21311281621456146\n",
      "Loss is 0.21311163902282715\n",
      "Loss is 0.21311090886592865\n",
      "Loss is 0.21311010420322418\n",
      "Loss is 0.21310922503471375\n",
      "Loss is 0.21310831606388092\n",
      "Loss is 0.21310725808143616\n",
      "Loss is 0.21310655772686005\n",
      "Loss is 0.21310579776763916\n",
      "Loss is 0.2131049484014511\n",
      "Loss is 0.21310411393642426\n",
      "Loss is 0.2131032645702362\n",
      "Loss is 0.21310225129127502\n",
      "Loss is 0.21310147643089294\n",
      "Loss is 0.2131006270647049\n",
      "Loss is 0.21309992671012878\n",
      "Loss is 0.2130991816520691\n",
      "Loss is 0.21309849619865417\n",
      "Loss is 0.21309754252433777\n",
      "Loss is 0.21309666335582733\n",
      "Loss is 0.21309609711170197\n",
      "Loss is 0.21309532225131989\n",
      "Loss is 0.2130947858095169\n",
      "Loss is 0.21309401094913483\n",
      "Loss is 0.2130933403968811\n",
      "Loss is 0.2130925953388214\n",
      "Loss is 0.2130916565656662\n",
      "Loss is 0.21309112012386322\n",
      "Loss is 0.21309037506580353\n",
      "Loss is 0.21308977901935577\n",
      "Loss is 0.2130890190601349\n",
      "Loss is 0.21308809518814087\n",
      "Loss is 0.21308761835098267\n",
      "Loss is 0.21308699250221252\n",
      "Loss is 0.21308626234531403\n",
      "Loss is 0.2130856215953827\n",
      "Loss is 0.21308515965938568\n",
      "Loss is 0.21308445930480957\n",
      "Loss is 0.21308371424674988\n",
      "Loss is 0.21308301389217377\n",
      "Loss is 0.2130824774503708\n",
      "Loss is 0.2130817472934723\n",
      "Loss is 0.21308134496212006\n",
      "Loss is 0.2130805402994156\n",
      "Loss is 0.21308010816574097\n",
      "Loss is 0.2130795270204544\n",
      "Loss is 0.21307872235774994\n",
      "Loss is 0.21307821571826935\n",
      "Loss is 0.21307754516601562\n",
      "Loss is 0.21307715773582458\n",
      "Loss is 0.21307647228240967\n",
      "Loss is 0.21307599544525146\n",
      "Loss is 0.2130753993988037\n",
      "Loss is 0.21307523548603058\n",
      "Loss is 0.21307462453842163\n",
      "Loss is 0.21307392418384552\n",
      "Loss is 0.21307334303855896\n",
      "Loss is 0.21307268738746643\n",
      "Loss is 0.21307213604450226\n",
      "Loss is 0.2130720168352127\n",
      "Loss is 0.21307113766670227\n",
      "Loss is 0.21307067573070526\n",
      "Loss is 0.2130703628063202\n",
      "Loss is 0.21306943893432617\n",
      "Loss is 0.21306926012039185\n",
      "Loss is 0.21306870877742767\n",
      "Loss is 0.21306823194026947\n",
      "Loss is 0.21306760609149933\n",
      "Loss is 0.2130674570798874\n",
      "Loss is 0.21306677162647247\n",
      "Loss is 0.21306639909744263\n",
      "Loss is 0.2130659818649292\n",
      "Loss is 0.21306537091732025\n",
      "Loss is 0.2130647450685501\n",
      "Loss is 0.21306470036506653\n",
      "Loss is 0.2130640596151352\n",
      "Loss is 0.21306361258029938\n",
      "Loss is 0.2130632996559143\n",
      "Loss is 0.21306276321411133\n",
      "Loss is 0.21306250989437103\n",
      "Loss is 0.21306200325489044\n",
      "Loss is 0.2130613774061203\n",
      "Loss is 0.21306128799915314\n",
      "Loss is 0.21306082606315613\n",
      "Loss is 0.21306008100509644\n",
      "Loss is 0.21305957436561584\n",
      "Loss is 0.21305927634239197\n",
      "Loss is 0.213058739900589\n",
      "Loss is 0.21305833756923676\n",
      "Loss is 0.21305808424949646\n",
      "Loss is 0.21305765211582184\n",
      "Loss is 0.21305736899375916\n",
      "Loss is 0.21305698156356812\n",
      "Loss is 0.21305663883686066\n",
      "Loss is 0.2130562961101532\n",
      "Loss is 0.21305590867996216\n",
      "Loss is 0.21305547654628754\n",
      "Loss is 0.21305619180202484\n",
      "Loss is 0.2130548655986786\n",
      "Loss is 0.21305419504642487\n",
      "Loss is 0.21305401623249054\n",
      "Loss is 0.21305373311042786\n",
      "Loss is 0.21305342018604279\n",
      "Loss is 0.21305327117443085\n",
      "Loss is 0.21305279433727264\n",
      "Loss is 0.2130524218082428\n",
      "Loss is 0.21305206418037415\n",
      "Loss is 0.21305175125598907\n",
      "Loss is 0.21305149793624878\n",
      "Loss is 0.21305087208747864\n",
      "Loss is 0.21305063366889954\n",
      "Loss is 0.21305032074451447\n",
      "Loss is 0.2130504846572876\n",
      "Loss is 0.21304966509342194\n",
      "Loss is 0.2130494862794876\n",
      "Loss is 0.2130492925643921\n",
      "Loss is 0.21304863691329956\n",
      "Loss is 0.21304842829704285\n",
      "Loss is 0.2130488008260727\n",
      "Loss is 0.2130480855703354\n",
      "Loss is 0.21304737031459808\n",
      "Loss is 0.2130473405122757\n",
      "Loss is 0.21304692327976227\n",
      "Loss is 0.21304665505886078\n",
      "Loss is 0.21304664015769958\n",
      "Loss is 0.21304593980312347\n",
      "Loss is 0.21304559707641602\n",
      "Loss is 0.21304552257061005\n",
      "Loss is 0.2130451798439026\n",
      "Loss is 0.21304480731487274\n",
      "Loss is 0.21304509043693542\n",
      "Loss is 0.2130446434020996\n",
      "Loss is 0.2130441516637802\n",
      "Loss is 0.21304373443126678\n",
      "Loss is 0.2130434662103653\n",
      "Loss is 0.21304336190223694\n",
      "Loss is 0.21304304897785187\n",
      "Loss is 0.21304281055927277\n",
      "Loss is 0.21304260194301605\n",
      "Loss is 0.21304230391979218\n",
      "Loss is 0.21304187178611755\n",
      "Loss is 0.21304166316986084\n",
      "Loss is 0.2130415141582489\n",
      "Loss is 0.21304132044315338\n",
      "Loss is 0.21304111182689667\n",
      "Loss is 0.21304108202457428\n",
      "Loss is 0.21304039657115936\n",
      "Loss is 0.21304015815258026\n",
      "Loss is 0.21304045617580414\n",
      "Loss is 0.21304018795490265\n",
      "Loss is 0.2130396068096161\n",
      "Loss is 0.21303927898406982\n",
      "Loss is 0.21304075419902802\n",
      "Loss is 0.2130398154258728\n",
      "Loss is 0.21303871273994446\n",
      "Loss is 0.21303848922252655\n",
      "Loss is 0.2130383998155594\n",
      "Loss is 0.21303805708885193\n",
      "Loss is 0.2130378633737564\n",
      "Loss is 0.21303752064704895\n",
      "Loss is 0.2130378633737564\n",
      "Loss is 0.21303711831569672\n",
      "Loss is 0.21303685009479523\n",
      "Loss is 0.21303673088550568\n",
      "Loss is 0.21303631365299225\n",
      "Loss is 0.2130362093448639\n",
      "Loss is 0.21303589642047882\n",
      "Loss is 0.21303577721118927\n",
      "Loss is 0.21303559839725494\n",
      "Loss is 0.21303555369377136\n",
      "Loss is 0.21303512156009674\n",
      "Loss is 0.21303494274616241\n",
      "Loss is 0.21303462982177734\n",
      "Loss is 0.21303533017635345\n",
      "Loss is 0.21303501725196838\n",
      "Loss is 0.2130344957113266\n",
      "Loss is 0.21303419768810272\n",
      "Loss is 0.21303404867649078\n",
      "Loss is 0.21303415298461914\n",
      "Loss is 0.21303367614746094\n",
      "Loss is 0.2130332589149475\n",
      "Loss is 0.2130327969789505\n",
      "Loss is 0.21303331851959229\n",
      "Loss is 0.21303267776966095\n",
      "Loss is 0.21303224563598633\n",
      "Loss is 0.2130325585603714\n",
      "Loss is 0.21303248405456543\n",
      "Loss is 0.21303193271160126\n",
      "Loss is 0.21303369104862213\n",
      "Loss is 0.2130327671766281\n",
      "Loss is 0.2130315601825714\n",
      "Loss is 0.21303139626979828\n",
      "Loss is 0.21303093433380127\n",
      "Loss is 0.2130308896303177\n",
      "Loss is 0.21303169429302216\n",
      "Loss is 0.21303114295005798\n",
      "Loss is 0.21303056180477142\n",
      "Loss is 0.2130305916070938\n",
      "Loss is 0.21303029358386993\n",
      "Loss is 0.2130299210548401\n",
      "Loss is 0.21303071081638336\n",
      "Loss is 0.21303053200244904\n",
      "Loss is 0.21302957832813263\n",
      "Loss is 0.21302951872348785\n",
      "Loss is 0.21302933990955353\n",
      "Loss is 0.21302944421768188\n",
      "Loss is 0.2130291759967804\n",
      "Loss is 0.21302887797355652\n",
      "Loss is 0.21302852034568787\n",
      "Loss is 0.21302857995033264\n",
      "Loss is 0.21302855014801025\n",
      "Loss is 0.21302823722362518\n",
      "Loss is 0.21302802860736847\n",
      "Loss is 0.21302837133407593\n",
      "Loss is 0.21302765607833862\n",
      "Loss is 0.21302761137485504\n",
      "Loss is 0.21302883327007294\n",
      "Loss is 0.2130277305841446\n",
      "Loss is 0.2130274474620819\n",
      "Loss is 0.213027223944664\n",
      "Loss is 0.2130270153284073\n",
      "Loss is 0.213026762008667\n",
      "Loss is 0.2130274921655655\n",
      "Loss is 0.2130269557237625\n",
      "Loss is 0.21302667260169983\n",
      "Loss is 0.2130262851715088\n",
      "Loss is 0.21302595734596252\n",
      "Loss is 0.21302860975265503\n",
      "Loss is 0.21302689611911774\n",
      "Loss is 0.2130257934331894\n",
      "Loss is 0.21302592754364014\n",
      "Loss is 0.21302565932273865\n",
      "Loss is 0.21302530169487\n",
      "Loss is 0.21303029358386993\n",
      "Loss is 0.21302609145641327\n",
      "Loss is 0.21302522718906403\n",
      "Loss is 0.21302518248558044\n",
      "Loss is 0.2130248248577118\n",
      "Loss is 0.21302476525306702\n",
      "Loss is 0.2130245715379715\n",
      "Loss is 0.21302443742752075\n",
      "Loss is 0.2130242884159088\n",
      "Loss is 0.21302402019500732\n",
      "Loss is 0.21302391588687897\n",
      "Loss is 0.21302403509616852\n",
      "Loss is 0.21302378177642822\n",
      "Loss is 0.21302376687526703\n",
      "Loss is 0.21302393078804016\n",
      "Loss is 0.2130233347415924\n",
      "Loss is 0.21302305161952972\n",
      "Loss is 0.21302393078804016\n",
      "Loss is 0.21302299201488495\n",
      "Loss is 0.2130228877067566\n",
      "Loss is 0.2130243182182312\n",
      "Loss is 0.2130233198404312\n",
      "Loss is 0.2130228877067566\n",
      "Loss is 0.2130226343870163\n",
      "Loss is 0.21302230656147003\n",
      "Loss is 0.21302412450313568\n",
      "Loss is 0.21302346885204315\n",
      "Loss is 0.21302226185798645\n",
      "Loss is 0.21302196383476257\n",
      "Loss is 0.21302209794521332\n",
      "Loss is 0.21302184462547302\n",
      "Loss is 0.21302206814289093\n",
      "Loss is 0.21302157640457153\n",
      "Loss is 0.21302199363708496\n",
      "Loss is 0.2130214124917984\n",
      "Loss is 0.21302178502082825\n",
      "Loss is 0.21302127838134766\n",
      "Loss is 0.21302367746829987\n",
      "Loss is 0.21302218735218048\n",
      "Loss is 0.21302132308483124\n",
      "Loss is 0.21302123367786407\n",
      "Loss is 0.21302087604999542\n",
      "Loss is 0.21302925050258636\n",
      "Loss is 0.2130209058523178\n",
      "Loss is 0.21302075684070587\n",
      "Loss is 0.21302050352096558\n",
      "Loss is 0.21302039921283722\n",
      "Loss is 0.21302036941051483\n",
      "Loss is 0.21302028000354767\n",
      "Loss is 0.21302007138729095\n",
      "Loss is 0.21302002668380737\n",
      "Loss is 0.2130196988582611\n",
      "Loss is 0.21301957964897156\n",
      "Loss is 0.21301959455013275\n",
      "Loss is 0.21301943063735962\n",
      "Loss is 0.21301943063735962\n",
      "Loss is 0.2130192667245865\n",
      "Loss is 0.2130192518234253\n",
      "Loss is 0.21301931142807007\n",
      "Loss is 0.2130192220211029\n",
      "Loss is 0.2130189836025238\n",
      "Loss is 0.21301932632923126\n",
      "Loss is 0.21301932632923126\n",
      "Loss is 0.21301928162574768\n",
      "Loss is 0.21301938593387604\n",
      "Loss is 0.21301886439323425\n",
      "Loss is 0.21301859617233276\n",
      "Loss is 0.21301861107349396\n",
      "Loss is 0.21301837265491486\n",
      "Loss is 0.2130182832479477\n",
      "Loss is 0.2130182981491089\n",
      "Loss is 0.21301816403865814\n",
      "Loss is 0.2130184769630432\n",
      "Loss is 0.21301820874214172\n",
      "Loss is 0.21301805973052979\n",
      "Loss is 0.21301789581775665\n",
      "Loss is 0.21301788091659546\n",
      "Loss is 0.21301884949207306\n",
      "Loss is 0.21301960945129395\n",
      "Loss is 0.21301816403865814\n",
      "Loss is 0.21301788091659546\n",
      "Loss is 0.2130175679922104\n",
      "Loss is 0.21301744878292084\n",
      "Loss is 0.2130173295736313\n",
      "Loss is 0.21301761269569397\n",
      "Loss is 0.21301725506782532\n",
      "Loss is 0.21301721036434174\n",
      "Loss is 0.21301744878292084\n",
      "Loss is 0.21301805973052979\n",
      "Loss is 0.21301697194576263\n",
      "Loss is 0.21301697194576263\n",
      "Loss is 0.2130175083875656\n",
      "Loss is 0.21301673352718353\n",
      "Loss is 0.21301676332950592\n",
      "Loss is 0.2130165845155716\n",
      "Loss is 0.21301692724227905\n",
      "Loss is 0.21301637589931488\n",
      "Loss is 0.2130173295736313\n",
      "Loss is 0.21301645040512085\n",
      "Loss is 0.21301841735839844\n",
      "Loss is 0.21301697194576263\n",
      "Loss is 0.21301639080047607\n",
      "Loss is 0.21301603317260742\n",
      "Loss is 0.213017076253891\n",
      "Loss is 0.21301621198654175\n",
      "Loss is 0.2130165547132492\n",
      "Loss is 0.21301639080047607\n",
      "Loss is 0.21301603317260742\n",
      "Loss is 0.21301579475402832\n",
      "Loss is 0.21301594376564026\n",
      "Loss is 0.2130153328180313\n",
      "Loss is 0.2130158543586731\n",
      "Loss is 0.21301540732383728\n",
      "Loss is 0.21301543712615967\n",
      "Loss is 0.21301549673080444\n",
      "Loss is 0.21301594376564026\n",
      "Loss is 0.21301546692848206\n",
      "Loss is 0.21301506459712982\n",
      "Loss is 0.21301622688770294\n",
      "Loss is 0.21301491558551788\n",
      "Loss is 0.2130148559808731\n",
      "Loss is 0.21301491558551788\n",
      "Loss is 0.21301484107971191\n",
      "Loss is 0.21301482617855072\n",
      "Loss is 0.21303172409534454\n",
      "Loss is 0.21301709115505219\n",
      "Loss is 0.21301555633544922\n",
      "Loss is 0.2130148708820343\n",
      "Loss is 0.21301454305648804\n",
      "Loss is 0.21301445364952087\n",
      "Loss is 0.21301449835300446\n",
      "Loss is 0.21301446855068207\n",
      "Loss is 0.2130143940448761\n",
      "Loss is 0.21301431953907013\n",
      "Loss is 0.21301425993442535\n",
      "Loss is 0.2130141407251358\n",
      "Loss is 0.21301408112049103\n",
      "Loss is 0.21301399171352386\n",
      "Loss is 0.21301387250423431\n",
      "Loss is 0.21301400661468506\n",
      "Loss is 0.21301381289958954\n",
      "Loss is 0.2130139172077179\n",
      "Loss is 0.21301425993442535\n",
      "Loss is 0.2130139172077179\n",
      "Loss is 0.21301372349262238\n",
      "Loss is 0.21301378309726715\n",
      "Loss is 0.21301423013210297\n",
      "Loss is 0.21301372349262238\n",
      "Loss is 0.21301385760307312\n",
      "Loss is 0.2130134105682373\n",
      "Loss is 0.21301411092281342\n",
      "Loss is 0.21301349997520447\n",
      "Loss is 0.21301357448101044\n",
      "Loss is 0.21301379799842834\n",
      "Loss is 0.21301381289958954\n",
      "Loss is 0.21301335096359253\n",
      "Loss is 0.2130132019519806\n",
      "Loss is 0.2130134254693985\n",
      "Loss is 0.21301864087581635\n",
      "Loss is 0.21301309764385223\n",
      "Loss is 0.21301379799842834\n",
      "Loss is 0.2130129635334015\n",
      "Loss is 0.21301285922527313\n",
      "Loss is 0.21301275491714478\n",
      "Loss is 0.21301278471946716\n",
      "Loss is 0.2130127251148224\n",
      "Loss is 0.21301259100437164\n",
      "Loss is 0.21301274001598358\n",
      "Loss is 0.2130129486322403\n",
      "Loss is 0.2130129188299179\n",
      "Loss is 0.21301381289958954\n",
      "Loss is 0.21301285922527313\n",
      "Loss is 0.21301282942295074\n",
      "Loss is 0.2130126953125\n",
      "Loss is 0.21301239728927612\n",
      "Loss is 0.21301265060901642\n",
      "Loss is 0.2130124270915985\n",
      "Loss is 0.2130122184753418\n",
      "Loss is 0.21301282942295074\n",
      "Loss is 0.21301387250423431\n",
      "Loss is 0.21301202476024628\n",
      "Loss is 0.21301236748695374\n",
      "Loss is 0.21301206946372986\n",
      "Loss is 0.21301497519016266\n",
      "Loss is 0.21301226317882538\n",
      "Loss is 0.21301192045211792\n",
      "Loss is 0.21301206946372986\n",
      "Loss is 0.2130119949579239\n",
      "Loss is 0.2130119651556015\n",
      "Loss is 0.21301177144050598\n",
      "Loss is 0.21301190555095673\n",
      "Loss is 0.2130182832479477\n",
      "Loss is 0.21301327645778656\n",
      "Loss is 0.21301168203353882\n",
      "Loss is 0.21301160752773285\n",
      "Loss is 0.21301156282424927\n",
      "Loss is 0.21301144361495972\n",
      "Loss is 0.2130115032196045\n",
      "Loss is 0.2130114585161209\n",
      "Loss is 0.21301288902759552\n",
      "Loss is 0.21301142871379852\n",
      "Loss is 0.21301135420799255\n",
      "Loss is 0.21301136910915375\n",
      "Loss is 0.2130115032196045\n",
      "Loss is 0.2130110114812851\n",
      "Loss is 0.21301116049289703\n",
      "Loss is 0.21301151812076569\n",
      "Loss is 0.2130116969347\n",
      "Loss is 0.21301135420799255\n",
      "Loss is 0.21301081776618958\n",
      "Loss is 0.21301093697547913\n",
      "Loss is 0.21301983296871185\n",
      "Loss is 0.21301473677158356\n",
      "Loss is 0.21301071345806122\n",
      "Loss is 0.21301108598709106\n",
      "Loss is 0.21301090717315674\n",
      "Loss is 0.21301068365573883\n",
      "Loss is 0.21301060914993286\n",
      "Loss is 0.2130105197429657\n",
      "Loss is 0.21301040053367615\n",
      "Loss is 0.2130105346441269\n",
      "Loss is 0.2130109667778015\n",
      "Loss is 0.21301044523715973\n",
      "Loss is 0.21301071345806122\n",
      "Loss is 0.2130143642425537\n",
      "Loss is 0.21301516890525818\n",
      "Loss is 0.21301065385341644\n",
      "Loss is 0.21301031112670898\n",
      "Loss is 0.21301035583019257\n",
      "Loss is 0.2130102962255478\n",
      "Loss is 0.21301022171974182\n",
      "Loss is 0.21301023662090302\n",
      "Loss is 0.21301019191741943\n",
      "Loss is 0.21300996840000153\n",
      "Loss is 0.21300996840000153\n",
      "Loss is 0.2130100131034851\n",
      "Loss is 0.21300987899303436\n",
      "Loss is 0.21300998330116272\n",
      "Loss is 0.2130100131034851\n",
      "Loss is 0.2130100280046463\n",
      "Loss is 0.21300996840000153\n",
      "Loss is 0.2130098193883896\n",
      "Loss is 0.21300989389419556\n",
      "Loss is 0.21301071345806122\n",
      "Loss is 0.2130102664232254\n",
      "Loss is 0.21300971508026123\n",
      "Loss is 0.2130097597837448\n",
      "Loss is 0.2130107283592224\n",
      "Loss is 0.2130124717950821\n",
      "Loss is 0.213009774684906\n",
      "Loss is 0.21300971508026123\n",
      "Loss is 0.21300990879535675\n",
      "Loss is 0.21300961077213287\n",
      "Loss is 0.21300962567329407\n",
      "Loss is 0.21300946176052094\n",
      "Loss is 0.21300961077213287\n",
      "Loss is 0.21301066875457764\n",
      "Loss is 0.2130095362663269\n",
      "Loss is 0.21300937235355377\n",
      "Loss is 0.21300946176052094\n",
      "Loss is 0.21300971508026123\n",
      "Loss is 0.21300923824310303\n",
      "Loss is 0.21300916373729706\n",
      "Loss is 0.21301691234111786\n",
      "Loss is 0.2130155712366104\n",
      "Loss is 0.21300917863845825\n",
      "Loss is 0.2130100429058075\n",
      "Loss is 0.21300894021987915\n",
      "Loss is 0.21300916373729706\n",
      "Loss is 0.21300911903381348\n",
      "Loss is 0.21300898492336273\n",
      "Loss is 0.21300895512104034\n",
      "Loss is 0.2130088210105896\n",
      "Loss is 0.2130087912082672\n",
      "Loss is 0.2130088061094284\n",
      "Loss is 0.21300876140594482\n",
      "Loss is 0.2130088210105896\n",
      "Loss is 0.21300990879535675\n",
      "Loss is 0.21300867199897766\n",
      "Loss is 0.21300876140594482\n",
      "Loss is 0.21300865709781647\n",
      "Loss is 0.21300877630710602\n",
      "Loss is 0.21300877630710602\n",
      "Loss is 0.21300865709781647\n",
      "Loss is 0.2130090445280075\n",
      "Loss is 0.2130085825920105\n",
      "Loss is 0.21301612257957458\n",
      "Loss is 0.21301227807998657\n",
      "Loss is 0.21300923824310303\n",
      "Loss is 0.21300864219665527\n",
      "Loss is 0.21300876140594482\n",
      "Loss is 0.21300846338272095\n",
      "Loss is 0.21300850808620453\n",
      "Loss is 0.2130083590745926\n",
      "Loss is 0.21300843358039856\n",
      "Loss is 0.21300829946994781\n",
      "Loss is 0.21300829946994781\n",
      "Loss is 0.21301284432411194\n",
      "Loss is 0.2130100578069687\n",
      "Loss is 0.21300891041755676\n",
      "Loss is 0.213008314371109\n",
      "Loss is 0.21300822496414185\n",
      "Loss is 0.2130080908536911\n",
      "Loss is 0.2130080610513687\n",
      "Loss is 0.21300813555717468\n",
      "Loss is 0.2130107581615448\n",
      "Loss is 0.21300816535949707\n",
      "Loss is 0.213008314371109\n",
      "Loss is 0.21300829946994781\n",
      "Loss is 0.213008314371109\n",
      "Loss is 0.2130080908536911\n",
      "Loss is 0.2130083441734314\n",
      "Loss is 0.21300965547561646\n",
      "Loss is 0.2130081206560135\n",
      "Loss is 0.21300840377807617\n",
      "Loss is 0.2130080610513687\n",
      "Loss is 0.21300800144672394\n",
      "Loss is 0.2130090892314911\n",
      "Loss is 0.21301382780075073\n",
      "Loss is 0.21300970017910004\n",
      "Loss is 0.2130080759525299\n",
      "Loss is 0.21300791203975677\n",
      "Loss is 0.2130078673362732\n",
      "Loss is 0.21300791203975677\n",
      "Loss is 0.21300776302814484\n",
      "Loss is 0.21300780773162842\n",
      "Loss is 0.21300767362117767\n",
      "Loss is 0.21300798654556274\n",
      "Loss is 0.21301254630088806\n",
      "Loss is 0.21300828456878662\n",
      "Loss is 0.2130076289176941\n",
      "Loss is 0.21300765872001648\n",
      "Loss is 0.21300756931304932\n",
      "Loss is 0.21300755441188812\n",
      "Loss is 0.21300745010375977\n",
      "Loss is 0.21300756931304932\n",
      "Loss is 0.21300828456878662\n",
      "Loss is 0.21301396191120148\n",
      "Loss is 0.21300964057445526\n",
      "Loss is 0.21300843358039856\n",
      "Loss is 0.21300767362117767\n",
      "Loss is 0.21300765872001648\n",
      "Loss is 0.2130073606967926\n",
      "Loss is 0.21300753951072693\n",
      "Loss is 0.213007390499115\n",
      "Loss is 0.21300770342350006\n",
      "Loss is 0.2130129486322403\n",
      "Loss is 0.21300891041755676\n",
      "Loss is 0.21300779283046722\n",
      "Loss is 0.21300740540027618\n",
      "Loss is 0.21300733089447021\n",
      "Loss is 0.21300727128982544\n",
      "Loss is 0.2130073457956314\n",
      "Loss is 0.2130073606967926\n",
      "Loss is 0.21300922334194183\n",
      "Loss is 0.21300770342350006\n",
      "Loss is 0.21300755441188812\n",
      "Loss is 0.21300792694091797\n",
      "Loss is 0.21300770342350006\n",
      "Loss is 0.21300728619098663\n",
      "Loss is 0.21300743520259857\n",
      "Loss is 0.21300911903381348\n",
      "Loss is 0.21300792694091797\n",
      "Loss is 0.21300818026065826\n",
      "Loss is 0.21300707757472992\n",
      "Loss is 0.2130073457956314\n",
      "Loss is 0.21300704777240753\n",
      "Loss is 0.21300700306892395\n",
      "Loss is 0.21300829946994781\n",
      "Loss is 0.21300959587097168\n",
      "Loss is 0.21300825476646423\n",
      "Loss is 0.21300703287124634\n",
      "Loss is 0.21300695836544037\n",
      "Loss is 0.21300706267356873\n",
      "Loss is 0.21300728619098663\n",
      "Loss is 0.21300727128982544\n",
      "Loss is 0.21300853788852692\n",
      "Loss is 0.21300822496414185\n",
      "Loss is 0.213007390499115\n",
      "Loss is 0.2130071073770523\n",
      "Loss is 0.21300667524337769\n",
      "Loss is 0.2130073606967926\n",
      "Loss is 0.21300996840000153\n",
      "Loss is 0.213006854057312\n",
      "Loss is 0.21300694346427917\n",
      "Loss is 0.21300682425498962\n",
      "Loss is 0.21300651133060455\n",
      "Loss is 0.2130066305398941\n",
      "Loss is 0.213007852435112\n",
      "Loss is 0.21300791203975677\n",
      "Loss is 0.21300674974918365\n",
      "Loss is 0.21300651133060455\n",
      "Loss is 0.2130064219236374\n",
      "Loss is 0.21300670504570007\n",
      "Loss is 0.21301135420799255\n",
      "Loss is 0.2130069136619568\n",
      "Loss is 0.21300648152828217\n",
      "Loss is 0.2130064070224762\n",
      "Loss is 0.21300627291202545\n",
      "Loss is 0.21300631761550903\n",
      "Loss is 0.21300633251667023\n",
      "Loss is 0.21300680935382843\n",
      "Loss is 0.2130064070224762\n",
      "Loss is 0.2130064219236374\n",
      "Loss is 0.21300886571407318\n",
      "Loss is 0.21300673484802246\n",
      "Loss is 0.2130068689584732\n",
      "Loss is 0.21300655603408813\n",
      "Loss is 0.2130061388015747\n",
      "Loss is 0.2130061537027359\n",
      "Loss is 0.21300646662712097\n",
      "Loss is 0.21300847828388214\n",
      "Loss is 0.21300680935382843\n",
      "Loss is 0.2130068987607956\n",
      "Loss is 0.21300645172595978\n",
      "Loss is 0.21300609409809113\n",
      "Loss is 0.2130061537027359\n",
      "Loss is 0.21300627291202545\n",
      "Loss is 0.2130085825920105\n",
      "Loss is 0.21300657093524933\n",
      "Loss is 0.213006854057312\n",
      "Loss is 0.21300624310970306\n",
      "Loss is 0.21300603449344635\n",
      "Loss is 0.21300604939460754\n",
      "Loss is 0.2130064070224762\n",
      "Loss is 0.21300944685935974\n",
      "Loss is 0.213005930185318\n",
      "Loss is 0.21300648152828217\n",
      "Loss is 0.2130061388015747\n",
      "Loss is 0.21300584077835083\n",
      "Loss is 0.2130059003829956\n",
      "Loss is 0.21300603449344635\n",
      "Loss is 0.21300822496414185\n",
      "Loss is 0.21300598978996277\n",
      "Loss is 0.21300585567951202\n",
      "Loss is 0.2130056470632553\n",
      "Loss is 0.2130059152841568\n",
      "Loss is 0.21300579607486725\n",
      "Loss is 0.21300587058067322\n",
      "Loss is 0.21300576627254486\n",
      "Loss is 0.21300587058067322\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# 超参数\n",
    "input_size = 5 # 一共五个特征\n",
    "hidden_size = 100 # 隐层神经元个数\n",
    "num_classes = 2 # 类别总数\n",
    "num_epochs = 10000 # 训练轮数\n",
    "#batch_size = 15 # 批处理大小\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 全连接神经网路\n",
    "class FCNetWork(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(FCNetWork,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size,num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out \n",
    "    \n",
    "    def predict(self,x):\n",
    "        pred = F.softmax(self.forward(x))\n",
    "        ans = []\n",
    "        for t in pred:\n",
    "            if t[0] > t[1]:\n",
    "                ans.append(0)\n",
    "            else:\n",
    "                ans.append(1)\n",
    "        return torch.tensor(ans)\n",
    "\n",
    "# class My_dataset(Dataset):\n",
    "#     def __init__(self,data,target):\n",
    "#         super().__init__()\n",
    "#         self.src = torch.tensor(data.values).type(torch.FloatTensor)\n",
    "#         self.trg = torch.tensor(target.values).type(LongTensor)\n",
    "    \n",
    "#     def __getitem__(self,index):\n",
    "#         return self.src[index],self.trg[index]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.src)\n",
    "\n",
    "\n",
    "model = FCNetWork(input_size,hidden_size,num_classes).to(device)\n",
    "\n",
    "# 确定损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "X = torch.tensor(train_set_data.values).type(torch.FloatTensor)\n",
    "y = torch.tensor(train_set_target.values).type(torch.LongTensor)\n",
    "\n",
    "losses = []\n",
    "for i in range(num_epochs):\n",
    "    y_pred = model.forward(X)\n",
    "    loss = criterion(y_pred,y.squeeze())\n",
    "    if i % 10 == 0:\n",
    "        print('Loss is {}'.format(loss.item()))\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# train_data = My_dataset(train_set_data,train_set_target)\n",
    "# test_data = My_dataset(test_set_data,test_set_target)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=train_data,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=test_data,\n",
    "#     batch_size=int(batch_size/3),\n",
    "#     shuffle=False\n",
    "# )\n",
    "# 开始训练模型\n",
    "# total_step = len(train_loader)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for i,(data,target) in enumerate(train_loader):\n",
    "#         data = data.to(device)\n",
    "#         target = target.to(device)\n",
    "\n",
    "#         # 向前传播\n",
    "#         out = model(data)\n",
    "#         loss = loss_func(out,target.squeeze())\n",
    "\n",
    "#         # 反向传播\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         if (i+1) % 5 == 0:\n",
    "#             print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                    .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6938775510204082\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(test_set_data.values).type(torch.FloatTensor)\n",
    "y = torch.tensor(test_set_target.values).type(torch.LongTensor)\n",
    "print(accuracy_score(model.predict(X),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}